{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bc93dd6-5dd4-424b-bad1-575eb4240973",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Firstly, we import all the tools we will be using throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eecd7eb0-d3e4-4a60-8997-29b3046cc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from matplotlib import pyplot as plt\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac4b54-a536-42ad-8c46-dcea9350613a",
   "metadata": {},
   "source": [
    "We download and build project dependencies. Do note that [git-lfs](https://git-lfs.com/) is required to clone the repos containing the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bfbe597-f309-41fd-bcbc-f11e50551950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'llama.cpp' already exists and is not an empty directory.\n",
      "I ccache not found. Consider installing it for faster compilation.\n",
      "I llama.cpp build info: \n",
      "I UNAME_S:   Linux\n",
      "I UNAME_P:   x86_64\n",
      "I UNAME_M:   x86_64\n",
      "I CFLAGS:    -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -fopenmp -Wdouble-promotion \n",
      "I CXXFLAGS:  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE \n",
      "I NVCCFLAGS: -std=c++11 -O3 \n",
      "I LDFLAGS:    \n",
      "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "I CXX:       c++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
      "\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/deprecation-warning/deprecation-warning.cpp -o examples/deprecation-warning/deprecation-warning.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE   examples/deprecation-warning/deprecation-warning.o -o main  \n",
      "NOTICE: The 'main' binary is deprecated. Please use 'llama-cli' instead.\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE  -c examples/deprecation-warning/deprecation-warning.cpp -o examples/deprecation-warning/deprecation-warning.o\n",
      "c++ -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread -fopenmp  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iggml/include -Iggml/src -Iinclude -Isrc -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_OPENMP -DGGML_USE_LLAMAFILE   examples/deprecation-warning/deprecation-warning.o -o server  \n",
      "NOTICE: The 'server' binary is deprecated. Please use 'llama-server' instead.\n"
     ]
    }
   ],
   "source": [
    "# llama.cpp\n",
    "! git clone https://github.com/ggerganov/llama.cpp.git\n",
    "! cd llama.cpp && make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a61957-8f43-4ab1-8c9d-b1ec74e6cf89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO MS COCO files, currently put manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2305e734-0670-431c-bc99-1d00b50465f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Requirements for llama.cpp Python utilities.\n",
    "CONDA = True\n",
    "if CONDA:\n",
    "    ! conda install --yes -c conda-forge sentencepiece==0.2.0 transformers==4.43.3 gguf==0.6.0 protobuf==4.25.3\n",
    "else:\n",
    "    ! pip install -r llama.cpp/examples/llava/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9b7014d-106e-4877-8912-393c48d15123",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "fatal: destination path 'llava-v1.5-7b' already exists and is not an empty directory.\n",
      "Error(s) during clone:\n",
      "`git clone` failed: exit status 128\n",
      "WARNING: `git lfs clone` is deprecated and will not be updated\n",
      "          with new flags from `git clone`\n",
      "\n",
      "`git clone` has been updated in upstream Git to have comparable\n",
      "speeds to `git lfs clone`.\n",
      "fatal: destination path 'clip-vit-large-patch14-336' already exists and is not an empty directory.\n",
      "Error(s) during clone:\n",
      "`git clone` failed: exit status 128\n",
      "Done!\n",
      "Now you can convert ../llava-v1.5-7b to a regular LLaMA GGUF file.\n",
      "Also, use ../llava-v1.5-7b/llava.projector to prepare a llava-encoder.gguf file.\n",
      "Projector tensors added\n",
      "\n",
      "skipping parameter: logit_scale\n",
      "skipping parameter: text_model.embeddings.token_embedding.weight\n",
      "skipping parameter: text_model.embeddings.position_embedding.weight\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.0.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.0.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.0.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.0.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.0.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.0.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.0.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.0.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.0.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.1.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.1.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.1.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.1.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.1.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.1.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.1.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.1.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.1.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.2.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.2.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.2.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.2.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.2.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.2.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.2.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.2.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.2.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.3.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.3.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.3.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.3.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.3.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.3.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.3.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.3.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.3.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.4.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.4.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.4.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.4.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.4.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.4.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.4.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.4.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.4.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.5.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.5.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.5.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.5.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.5.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.5.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.5.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.5.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.5.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.6.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.6.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.6.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.6.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.6.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.6.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.6.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.6.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.6.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.7.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.7.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.7.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.7.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.7.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.7.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.7.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.7.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.7.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.8.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.8.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.8.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.8.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.8.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.8.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.8.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.8.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.8.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.9.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.9.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.9.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.9.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.9.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.9.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.9.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.9.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.9.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.10.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.10.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.10.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.10.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.10.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.10.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.10.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.10.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.10.layer_norm2.bias\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.k_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.k_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.v_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.v_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.q_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.q_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.out_proj.weight\n",
      "skipping parameter: text_model.encoder.layers.11.self_attn.out_proj.bias\n",
      "skipping parameter: text_model.encoder.layers.11.layer_norm1.weight\n",
      "skipping parameter: text_model.encoder.layers.11.layer_norm1.bias\n",
      "skipping parameter: text_model.encoder.layers.11.mlp.fc1.weight\n",
      "skipping parameter: text_model.encoder.layers.11.mlp.fc1.bias\n",
      "skipping parameter: text_model.encoder.layers.11.mlp.fc2.weight\n",
      "skipping parameter: text_model.encoder.layers.11.mlp.fc2.bias\n",
      "skipping parameter: text_model.encoder.layers.11.layer_norm2.weight\n",
      "skipping parameter: text_model.encoder.layers.11.layer_norm2.bias\n",
      "skipping parameter: text_model.final_layer_norm.weight\n",
      "skipping parameter: text_model.final_layer_norm.bias\n",
      "  Converting to float32\n",
      "v.class_embd - f32 - shape = (1024,)\n",
      "tensor v.patch_embd.weight is always saved in f16\n",
      "v.patch_embd.weight - f16 - shape = (1024, 3, 14, 14)\n",
      "  Converting to float16\n",
      "v.position_embd.weight - f16 - shape = (577, 1024)\n",
      "  Converting to float32\n",
      "v.pre_ln.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.pre_ln.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.0.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.0.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.0.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.0.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.0.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.0.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.0.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.0.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.0.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.0.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.0.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.0.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.0.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.0.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.0.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.0.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.1.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.1.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.1.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.1.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.1.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.1.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.1.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.1.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.1.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.1.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.1.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.1.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.1.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.1.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.1.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.1.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.2.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.2.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.2.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.2.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.2.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.2.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.2.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.2.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.2.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.2.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.2.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.2.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.2.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.2.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.2.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.2.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.3.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.3.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.3.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.3.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.3.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.3.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.3.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.3.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.3.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.3.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.3.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.3.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.3.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.3.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.3.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.3.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.4.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.4.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.4.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.4.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.4.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.4.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.4.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.4.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.4.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.4.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.4.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.4.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.4.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.4.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.4.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.4.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.5.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.5.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.5.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.5.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.5.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.5.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.5.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.5.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.5.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.5.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.5.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.5.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.5.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.5.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.5.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.5.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.6.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.6.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.6.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.6.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.6.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.6.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.6.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.6.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.6.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.6.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.6.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.6.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.6.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.6.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.6.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.6.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.7.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.7.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.7.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.7.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.7.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.7.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.7.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.7.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.7.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.7.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.7.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.7.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.7.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.7.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.7.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.7.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.8.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.8.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.8.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.8.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.8.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.8.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.8.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.8.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.8.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.8.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.8.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.8.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.8.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.8.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.8.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.8.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.9.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.9.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.9.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.9.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.9.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.9.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.9.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.9.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.9.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.9.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.9.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.9.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.9.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.9.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.9.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.9.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.10.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.10.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.10.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.10.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.10.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.10.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.10.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.10.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.10.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.10.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.10.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.10.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.10.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.10.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.10.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.10.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.11.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.11.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.11.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.11.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.11.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.11.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.11.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.11.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.11.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.11.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.11.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.11.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.11.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.11.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.11.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.11.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.12.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.12.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.12.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.12.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.12.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.12.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.12.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.12.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.12.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.12.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.12.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.12.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.12.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.12.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.12.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.12.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.13.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.13.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.13.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.13.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.13.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.13.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.13.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.13.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.13.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.13.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.13.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.13.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.13.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.13.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.13.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.13.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.14.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.14.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.14.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.14.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.14.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.14.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.14.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.14.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.14.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.14.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.14.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.14.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.14.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.14.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.14.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.14.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.15.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.15.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.15.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.15.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.15.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.15.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.15.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.15.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.15.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.15.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.15.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.15.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.15.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.15.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.15.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.15.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.16.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.16.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.16.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.16.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.16.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.16.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.16.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.16.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.16.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.16.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.16.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.16.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.16.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.16.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.16.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.16.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.17.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.17.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.17.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.17.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.17.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.17.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.17.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.17.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.17.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.17.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.17.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.17.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.17.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.17.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.17.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.17.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.18.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.18.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.18.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.18.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.18.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.18.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.18.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.18.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.18.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.18.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.18.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.18.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.18.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.18.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.18.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.18.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.19.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.19.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.19.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.19.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.19.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.19.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.19.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.19.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.19.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.19.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.19.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.19.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.19.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.19.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.19.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.19.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.20.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.20.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.20.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.20.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.20.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.20.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.20.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.20.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.20.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.20.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.20.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.20.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.20.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.20.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.20.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.20.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.21.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.21.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.21.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.21.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.21.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.21.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.21.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.21.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.21.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.21.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.21.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.21.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.21.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.21.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.21.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.21.ln2.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.22.attn_k.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.22.attn_k.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.22.attn_v.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.22.attn_v.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.22.attn_q.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.22.attn_q.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.22.attn_out.weight - f16 - shape = (1024, 1024)\n",
      "  Converting to float32\n",
      "v.blk.22.attn_out.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.22.ln1.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.22.ln1.bias - f32 - shape = (1024,)\n",
      "  Converting to float16\n",
      "v.blk.22.ffn_down.weight - f16 - shape = (4096, 1024)\n",
      "  Converting to float32\n",
      "v.blk.22.ffn_down.bias - f32 - shape = (4096,)\n",
      "  Converting to float16\n",
      "v.blk.22.ffn_up.weight - f16 - shape = (1024, 4096)\n",
      "  Converting to float32\n",
      "v.blk.22.ffn_up.bias - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.22.ln2.weight - f32 - shape = (1024,)\n",
      "  Converting to float32\n",
      "v.blk.22.ln2.bias - f32 - shape = (1024,)\n",
      "skipping parameter: vision_model.post_layernorm.weight\n",
      "skipping parameter: vision_model.post_layernorm.bias\n",
      "skipping parameter: visual_projection.weight\n",
      "skipping parameter: text_projection.weight\n",
      "Done. Output file: ../llava-v1.5-7b/mmproj-model-f16.gguf\n"
     ]
    }
   ],
   "source": [
    "# LLaVA 1.5 7B\n",
    "! git lfs clone https://huggingface.co/liuhaotian/llava-v1.5-7b\n",
    "! git lfs clone https://huggingface.co/openai/clip-vit-large-patch14-336\n",
    "! cd llama.cpp && python ./examples/llava/llava_surgery.py -m ../llava-v1.5-7b\n",
    "! cd llama.cpp && python ./examples/llava/convert_image_encoder_to_gguf.py -m ../clip-vit-large-patch14-336 --llava-projector ../llava-v1.5-7b/llava.projector --output-dir ../llava-v1.5-7b\n",
    "! cd llama.cpp && python ./examples/convert_legacy_llama.py ../llava-v1.5-7b --skip-unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973427d7-188a-481a-9410-730efec6e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLaVa 1.6 7B\n",
    "! git lfs clone https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b\n",
    "! python llama.cpp/examples/llava/llava_surgery_v2.py -C -m llava-v1.6-vicuna-7b/\n",
    "! mkdir vit\n",
    "! cp llava-v1.6-vicuna-7b/llava.clip vit/pytorch_model.bin\n",
    "! cp llava-v1.6-vicuna-7b/llava.projector vit/\n",
    "! curl -s -q https://huggingface.co/cmp-nct/llava-1.6-gguf/raw/main/config_vit.json -o vit/config.json\n",
    "! python llama.cpp/examples/llava/convert_image_encoder_to_gguf.py -m vit --llava-projector vit/llava.projector --output-dir vit --clip-model-is-vision\n",
    "! python llama.cpp/examples/convert_legacy_llama.py llava-v1.6-vicuna-7b/ --skip-unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47f66e-b74f-4121-ba1d-ac613eeedf6d",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "Next, different models at different quantization models and methods are evaluated. Do note that CLIP image encoder quantization is not supported by llama.cpp, hence only quantizations of the language model are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8462d312-2a4c-4a96-ac74-090bb21fe857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(model, mmproj, seed=0, n_images=20, temp=0.1, prompt='\"Suggest a short caption for this image.\"'):\n",
    "    # ========================================================\n",
    "    # Set up validation dataset\n",
    "    # ========================================================\n",
    "    coco = COCO('coco/captions_val2014.json')\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "    ids = rng.choice(coco.getImgIds(), size=n_images)\n",
    "    imgs = coco.loadImgs(ids=ids)\n",
    "    anns = coco.loadAnns(coco.getAnnIds(imgIds=ids))\n",
    "    \n",
    "    # ========================================================\n",
    "    # Run and time individual image prompts\n",
    "    # ========================================================\n",
    "    data = []\n",
    "    for img in imgs:\n",
    "        path = 'coco/images/' + img['file_name']\n",
    "        if not os.path.isfile(path):\n",
    "            ! curl {img['coco_url']} >> {path}\n",
    "        t = timer()\n",
    "        result = ! llama.cpp/llama-llava-cli -m {model} --mmproj {mmproj} --image {path} --temp {temp} -s {seed} -p {prompt}\n",
    "        t = timer() - t\n",
    "        caption = result[-7].strip()\n",
    "        data += [{'image_id': img['id'], 'caption': caption, 'time': t}]\n",
    "        print(img['file_name'] + ' | ' + caption)\n",
    "\n",
    "    # ========================================================\n",
    "    # Evaluate responses\n",
    "    # ========================================================\n",
    "    with open('coco/results.json', 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "    cocoRes = coco.loadRes('coco/results.json')\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    # sorted(...) mitigates inconsistent behavior of individual metrics\n",
    "    cocoEval.params['image_id'] = sorted(cocoRes.getImgIds())\n",
    "    cocoEval.evaluate()\n",
    "    \n",
    "    # ========================================================\n",
    "    # Structure results\n",
    "    # ========================================================\n",
    "    df = pd.DataFrame(cocoEval.evalImgs).set_index('image_id')\n",
    "    df['SPICE All'] = [i['All']['pr'] for i in df['SPICE']]\n",
    "    df['SPICE Obj'] = [i['Object']['pr'] for i in df['SPICE']]\n",
    "    df['SPICE Rel'] = [i['Relation']['pr'] for i in df['SPICE']]\n",
    "    df = df.drop('SPICE', axis=1)\n",
    "    df = df.join(pd.DataFrame(data).set_index('image_id'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9726aac-9c46-465a-bd0f-0a3de57448d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "COCO_val2014_000000500473.jpg | A grocery store with a large produce section.\n",
      "COCO_val2014_000000150365.jpg | A giraffe stands in a field of tall grass.\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 112 tokens at 2869.85 tokens per second.\n",
      "PTBTokenizer tokenized 20 tokens at 469.65 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 17, 'reflen': 17, 'guess': [17, 15, 13, 11], 'correct': [14, 10, 5, 1]}\n",
      "ratio: 0.9999999999411765\n",
      "Bleu_1: 0.824\n",
      "Bleu_2: 0.741\n",
      "Bleu_3: 0.595\n",
      "Bleu_4: 0.372\n",
      "computing METEOR score...\n",
      "METEOR: 0.275\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.552\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.249\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.3 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.3 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.9 sec].\n",
      "Threads( StanfordCoreNLP ) [0.309 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 4.934 s\n",
      "SPICE: 0.180\n"
     ]
    }
   ],
   "source": [
    "df = experiment(\n",
    "    model='llava-v1.5-7b/llava-v1.5-7B-Q2_K.gguf',\n",
    "    mmproj='llava-v1.5-7b/mmproj-model-f16.gguf',\n",
    "    n_images=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f210c669-f4cd-4b71-995d-23ee4ebfe300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bleu_1</th>\n",
       "      <th>Bleu_2</th>\n",
       "      <th>Bleu_3</th>\n",
       "      <th>Bleu_4</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>ROUGE_L</th>\n",
       "      <th>CIDEr</th>\n",
       "      <th>SPICE All</th>\n",
       "      <th>SPICE Obj</th>\n",
       "      <th>SPICE Rel</th>\n",
       "      <th>caption</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150365</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.456671</td>\n",
       "      <td>0.000063</td>\n",
       "      <td>0.315950</td>\n",
       "      <td>0.521368</td>\n",
       "      <td>0.951239</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A giraffe stands in a field of tall grass.</td>\n",
       "      <td>29.202313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500473</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.92582</td>\n",
       "      <td>0.753947</td>\n",
       "      <td>0.541082</td>\n",
       "      <td>0.245304</td>\n",
       "      <td>0.582061</td>\n",
       "      <td>1.546057</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>A grocery store with a large produce section.</td>\n",
       "      <td>28.959107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Bleu_1  ...       time\n",
       "image_id            ...           \n",
       "150365    0.666667  ...  29.202313\n",
       "500473    1.000000  ...  28.959107\n",
       "\n",
       "[2 rows x 12 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df # If using spyder, consider using the variable viewer for color coded magnitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983d074a-0f6e-42e1-a013-83147e5db9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
